{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.path.join(os.getcwd(), 'transformers')\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from textrl import TextRLEnv, TextRLActor, train_agent_with_evaluation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# , torch_dtype=\"auto\", device_map=\"auto\"\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_path_1 = \"cardiffnlp/twitter-roberta-base-sentiment\" # 'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "sentiment_path_2 = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "sentiment_1 = pipeline('sentiment-analysis',model=sentiment_path_1,tokenizer=sentiment_path_1, top_k=None)\n",
    "sentiment_2 = pipeline('sentiment-analysis',model=sentiment_path_2,tokenizer=sentiment_path_2, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_1', 'score': 0.8726766705513},\n",
       "  {'label': 'LABEL_0', 'score': 0.0813116580247879},\n",
       "  {'label': 'LABEL_2', 'score': 0.046011634171009064}]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_1(\"\\xa0an (anonymous) (anonymous) (anonymous) (anonymous (anonymous) (anonymous)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': '1 star', 'score': 0.5140755772590637},\n",
       "  {'label': '2 stars', 'score': 0.22503311932086945},\n",
       "  {'label': '3 stars', 'score': 0.15606173872947693},\n",
       "  {'label': '4 stars', 'score': 0.0636562630534172},\n",
       "  {'label': '5 stars', 'score': 0.041173290461301804}]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_2(\"\\xa0an (anonymous) (anonymous) (anonymous) (anonymous (anonymous) (anonymous)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_score_nlptown(sentence, percent=1):\n",
    "    scores = sentiment_2(sentence)[0]\n",
    "    req_score = 0\n",
    "    for dict_inner in scores:\n",
    "        if dict_inner['label'] == '1 star':\n",
    "            req_score += percent * dict_inner['score']\n",
    "        if dict_inner['label'] == '2 star':\n",
    "            req_score += (1 - percent) * dict_inner['score']\n",
    "\n",
    "    return req_score\n",
    "\n",
    "def return_score_cardiff_roberta_xlm(sentence):\n",
    "    scores = sentiment_1(sentence)[0]\n",
    "    req_score = 0\n",
    "    for dict_inner in scores:\n",
    "        if dict_inner['label'] == 'negative':\n",
    "            req_score += 10 * dict_inner['score']\n",
    "        if dict_inner['label'] == 'positive':\n",
    "            req_score -= 10 * dict_inner['score']\n",
    "        if dict_inner['label'] == 'neutral':\n",
    "            req_score -= 5 * dict_inner['score']\n",
    "\n",
    "    return req_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class MyRLEnv(TextRLEnv):\n",
    "    def get_reward(self, input_item, predicted_list, finish):  # predicted will be the list of predicted token\n",
    "        rewards = []\n",
    "        for i in predicted_list:\n",
    "            if finish or len(i) > self.env_max_length:\n",
    "                predicted_text = tokenizer.convert_tokens_to_string(i)\n",
    "                # reward = 10 * bleu_score.sentence_bleu(references=[word_tokenize(input_item['output'])], hypothesis=word_tokenize(predicted_text))\n",
    "                # rewards.append(reward)\n",
    "                reward = return_score_cardiff_roberta_xlm(input_item['input']+predicted_text)\n",
    "                reward = min(reward, return_score_nlptown(input_item['input']+predicted_text))\n",
    "                rewards.append(reward)\n",
    "                # calculate reward score base on predicted_list\n",
    "            else:\n",
    "                rewards.append(0)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('newsCompiled.csv')\n",
    "df['ol'] = df.apply(lambda x: {'input': x['Input'], 'output': x['Output']}, axis=1)\n",
    "observation_list = df['ol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "observaton_list = [{\"input\": \"Dogecoin is \"}]\n",
    "env = MyRLEnv(\n",
    "    model, tokenizer, \n",
    "    observation_input=observation_list, \n",
    "    max_length=25, \n",
    "    compare_sample=5\n",
    ")\n",
    "actor = TextRLActor(\n",
    "    env, model, tokenizer, \n",
    "    act_deterministically=False, \n",
    "    optimizer='adamw',\n",
    "    temperature=0.8,\n",
    "    top_k=100,\n",
    "    top_p=0.85\n",
    ")\n",
    "# act_deterministically=False,\n",
    "# temperature=1.0,\n",
    "# top_k=0,\n",
    "# top_p=1.0,\n",
    "# repetition_penalty=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed official says weak data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' from China will lead to lower interest rates.\\n\\n\\nChina has been increasing its interest rate for the last few months,',\n",
       " \" would be 'deadly for the government'\\n\\n\\nThe government has announced a $10bn budget for 2015-16\",\n",
       " ' is to blame for lack of progress\\n\\n\\nMOSCOW (Sputnik) — Russian Foreign Ministry spokesman Sergei',\n",
       " ' and sluggish economic growth have helped make the move to buy Chinese bonds.\\n\\n\\n\"The market is not ready to give',\n",
       " \" can be used to limit Russia's nuclear program\\n\\n\\nThe country is building a new nuclear reactor to replace a damaged one\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = actor.agent_ppo(\n",
    "    update_interval=2, \n",
    "    minibatch_size=2, \n",
    "    epochs=10,\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "test_phrase = observation_list[0]['input']\n",
    "print(test_phrase)\n",
    "actor.predict({'input': test_phrase})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd1/venkat/RL/Text_Generation/venv/lib/python3.7/site-packages/pfrl/agents/ppo.py:133: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  actions = torch.tensor([b[\"action\"] for b in dataset], device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:bloom—test-3 step:27 episode:0 R:0\n",
      "statistics:[('average_value', -0.7015761), ('average_entropy', 1.4301476), ('average_value_loss', 0.1687923411326483), ('average_policy_loss', -0.13913709436356989), ('n_updates', 130), ('explained_variance', 0.7841575385543729)]\n",
      "evaluation episode 0 length:27 R:0\n",
      "evaluation episode 1 length:27 R:0\n",
      "evaluation episode 2 length:27 R:0\n",
      "evaluation episode 3 length:27 R:0\n",
      "evaluation episode 4 length:27 R:0\n",
      "The best score is updated -3.4028235e+38 -> 0.0\n",
      "Saved the agent to bloom—test-3/best\n",
      "outdir:bloom—test-3 step:54 episode:1 R:0\n",
      "statistics:[('average_value', -0.118922256), ('average_entropy', 1.0440704), ('average_value_loss', 0.16484971854835748), ('average_policy_loss', -0.08675848134048252), ('n_updates', 270), ('explained_variance', -81.05598398428091)]\n",
      "evaluation episode 0 length:27 R:0\n",
      "evaluation episode 1 length:27 R:0\n",
      "evaluation episode 2 length:27 R:0\n",
      "evaluation episode 3 length:27 R:0\n",
      "evaluation episode 4 length:27 R:0\n",
      "outdir:bloom—test-3 step:81 episode:2 R:0\n",
      "statistics:[('average_value', 0.16733886), ('average_entropy', 0.8581701), ('average_value_loss', 0.05740455741994083), ('average_policy_loss', -0.0681951494831589), ('n_updates', 400), ('explained_variance', -4.441601598863723)]\n",
      "evaluation episode 0 length:27 R:0\n",
      "evaluation episode 1 length:27 R:0\n",
      "evaluation episode 2 length:27 R:0\n",
      "evaluation episode 3 length:27 R:0\n",
      "evaluation episode 4 length:27 R:0\n",
      "outdir:bloom—test-3 step:100 episode:3 R:0\n",
      "statistics:[('average_value', 0.16058105), ('average_entropy', 0.76002496), ('average_value_loss', 0.01793369373306632), ('average_policy_loss', -0.03996437291009352), ('n_updates', 500), ('explained_variance', 0.01306295044435224)]\n",
      "evaluation episode 0 length:27 R:0\n",
      "evaluation episode 1 length:27 R:0\n",
      "evaluation episode 2 length:27 R:0\n",
      "evaluation episode 3 length:27 R:0\n",
      "evaluation episode 4 length:27 R:0\n",
      "Saved the agent to bloom—test-3/100_finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<textrl.actor.TextPPO at 0x7fcc72910eb8>,\n",
       " [{'average_value': -0.7015761,\n",
       "   'average_entropy': 1.4301476,\n",
       "   'average_value_loss': 0.1687923411326483,\n",
       "   'average_policy_loss': -0.13913709436356989,\n",
       "   'n_updates': 130,\n",
       "   'explained_variance': 0.7841575385543729,\n",
       "   'eval_score': 0.0},\n",
       "  {'average_value': -0.118922256,\n",
       "   'average_entropy': 1.0440704,\n",
       "   'average_value_loss': 0.16484971854835748,\n",
       "   'average_policy_loss': -0.08675848134048252,\n",
       "   'n_updates': 270,\n",
       "   'explained_variance': -81.05598398428091,\n",
       "   'eval_score': 0.0},\n",
       "  {'average_value': 0.16733886,\n",
       "   'average_entropy': 0.8581701,\n",
       "   'average_value_loss': 0.05740455741994083,\n",
       "   'average_policy_loss': -0.0681951494831589,\n",
       "   'n_updates': 400,\n",
       "   'explained_variance': -4.441601598863723,\n",
       "   'eval_score': 0.0},\n",
       "  {'average_value': 0.16058105,\n",
       "   'average_entropy': 0.76002496,\n",
       "   'average_value_loss': 0.01793369373306632,\n",
       "   'average_policy_loss': -0.03996437291009352,\n",
       "   'n_updates': 500,\n",
       "   'explained_variance': 0.01306295044435224,\n",
       "   'eval_score': 0.0}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=100,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=5,\n",
    "    eval_interval=25,\n",
    "    outdir='bloom—test-3',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Fed official says weak data', 'output': 'caused by weather, should not slow taper'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' has been a factor\\n\\n\\nThere has been a strong rally in support of a proposal to place a $1 trillion tax',\n",
       " ' has been found\\n\\n\\nGDP has been about 30% more in recent years\\n\\n\\nGDP has been about',\n",
       " ' has been a factor\\n\\n\\nIf this has been a type of type of type of type of type which has been a',\n",
       " \" has been used to blame Russia\\n\\n\\n'We may have been misled about this type of research which has been conducted in\",\n",
       " ' has been a factor in some of the worst financial crisis in recent memory\\n\\n\\nMore than a third of about 5 million']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load('./bloom—test-3/best')\n",
    "print(observation_list[0])\n",
    "actor.predict(observation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
